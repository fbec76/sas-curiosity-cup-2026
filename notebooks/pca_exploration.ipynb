{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:11:02.568739Z",
     "start_time": "2026-02-15T19:11:02.260637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import requests\n",
    "import os"
   ],
   "id": "a0cdad6056258951",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:11:02.590165Z",
     "start_time": "2026-02-15T19:11:02.569396Z"
    }
   },
   "cell_type": "code",
   "source": "RANDOM_STATE = 42",
   "id": "c73ab258d846fce1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:40.336730Z",
     "start_time": "2026-02-15T19:12:33.202018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set to True to download data from GitHub, False to load from local processed directory\n",
    "DOWNLOAD = False\n",
    "\n",
    "if DOWNLOAD:\n",
    "    # Download CSV files from GitHub repository\n",
    "    directory = \"./downloads/\"\n",
    "    filenames = [\n",
    "        \"master_data.parquet\",\n",
    "        \"master_data_dropped.parquet\",\n",
    "        \"master_data_imputed.parquet\"\n",
    "    ]\n",
    "\n",
    "    # Common URL parts\n",
    "    base_url = \"https://github.com/fbec76/sas-curiosity-cup-2026/raw/refs/heads/main/datasets/processed/\"\n",
    "    for fname in filenames:\n",
    "        url = base_url + fname + \"?download=\"\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            with open(directory + fname, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: {fname}\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {fname}\")\n",
    "    data_dir = \"./downloads/\"\n",
    "    master_data_orig = pd.read_parquet(data_dir + \"master_data.parquet\")\n",
    "    master_data_dropped_orig = pd.read_parquet(data_dir + \"master_data_dropped.parquet\")\n",
    "    master_data_imputed_orig = pd.read_parquet(data_dir + \"master_data_imputed.parquet\")\n",
    "    print(\"Data loaded from downloads directory.\")\n",
    "else:\n",
    "    master_data_orig = pd.read_parquet(\"../datasets/processed/master_data.parquet\")\n",
    "    master_data_dropped_orig = pd.read_parquet(\"../datasets/processed/master_data_dropped.parquet\")\n",
    "    master_data_imputed_orig = pd.read_parquet(\"../datasets/processed/master_data_imputed.parquet\")\n",
    "    data_dir = \"../datasets/processed/\"\n",
    "    print(\"Data loaded from processed directory.\")\n"
   ],
   "id": "31e0289e580d784b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from processed directory.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:45.099017Z",
     "start_time": "2026-02-15T19:12:40.339375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "master_data = master_data_orig.copy()\n",
    "master_data_dropped = master_data_dropped_orig.copy()\n",
    "master_data_imputed = master_data_imputed_orig.copy()"
   ],
   "id": "b03602b223f1e61a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:46.272564Z",
     "start_time": "2026-02-15T19:12:45.192582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop GAME_DATE column from master_data_dropped and master_data_imputed\n",
    "master_data_dropped = master_data_dropped.drop(columns=[\"GAME_DATE\"])\n",
    "master_data_imputed = master_data_imputed.drop(columns=[\"GAME_DATE\"])\n",
    "\n",
    "# conver POS_ columns to boolean in master_data_dropped\n",
    "pos_columns = [col for col in master_data_dropped.columns if col.startswith(\"POS_\")]\n",
    "master_data_dropped[pos_columns] = master_data_dropped[pos_columns].astype(bool)"
   ],
   "id": "5cd4136d63653d29",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:48.233220Z",
     "start_time": "2026-02-15T19:12:46.294046Z"
    }
   },
   "source": [
    "# define target variable and features list\n",
    "target_variable = \"MADE_SHOT\"\n",
    "\n",
    "# features list for master_data_dropped\n",
    "cols_to_exclude = [\n",
    "    target_variable, \"GAME_ID\", \"PLAYER_ID\", \"PLAYER_NAME\", \"TEAM_ID\", \"TEAM_NAME\",\n",
    "    \"LAT\", \"LON\", \"D_LAT\", \"D_LON\", \"FLIGHT_TIME_MIN\", \"HOME_TEAM\", \"AWAY_TEAM\", \"IS_3PT\"\n",
    "]\n",
    "\n",
    "features_list = master_data_dropped.columns.difference(cols_to_exclude).tolist()\n",
    "\n",
    "features_list_dropped = features_list\n",
    "features_list_imputed = [col for col in features_list if not col.startswith(\"POS_\")]\n",
    "\n",
    "# Continuous features to normalize (z-score)\n",
    "continuous_features = [\n",
    "    \"BODY_FAT_PCT\",\n",
    "    \"DISTANCE_KM\",\n",
    "    \"HAND_LENGTH_CM\",\n",
    "    \"HAND_WIDTH_CM\",\n",
    "    \"HEIGHT_CM\",\n",
    "    \"LANE_AGILITY_TIME_S\",\n",
    "    \"LOC_X_CM\",\n",
    "    \"LOC_Y_CM\",\n",
    "    \"MAX_VERTICAL_LEAP_CM\",\n",
    "    \"REST_D\",\n",
    "    \"SEASON\",  # normalize to allow learning an overall trend\n",
    "    \"SHOT_DISTANCE_CM\",\n",
    "    \"STANDING_REACH_CM\",\n",
    "    \"STANDING_VERTICAL_LEAP_CM\",\n",
    "    \"THREE_QUARTER_SPRINT_S\",\n",
    "    \"TIME_LEFT_S\",\n",
    "    \"TZ_SHIFT\",\n",
    "    \"WEIGHT_KG\",\n",
    "    \"WINGSPAN_CM\",\n",
    "]\n",
    "\n",
    "# One-hot encode QUARTER\n",
    "if \"QUARTER\" in master_data_dropped.columns:\n",
    "    master_data_dropped = pd.get_dummies(\n",
    "        master_data_dropped,\n",
    "        columns=[\"QUARTER\"],\n",
    "        prefix=\"QUARTER\",\n",
    "        drop_first=False\n",
    "    )\n",
    "\n",
    "if \"QUARTER\" in master_data_imputed.columns:\n",
    "    master_data_imputed = pd.get_dummies(\n",
    "        master_data_imputed,\n",
    "        columns=[\"QUARTER\"],\n",
    "        prefix=\"QUARTER\",\n",
    "        drop_first=False\n",
    "    )\n",
    "\n",
    "# Ensure we only scale columns that exist (robust to missing columns)\n",
    "continuous_features_present = [c for c in continuous_features if c in master_data_dropped.columns]\n",
    "\n",
    "# Rebuild features_list after one-hot encoding QUARTER (new columns added)\n",
    "features_list = master_data_dropped.columns.difference(cols_to_exclude).tolist()\n",
    "\n",
    "features_list_dropped = features_list\n",
    "features_list_imputed = [col for col in features_list if not col.startswith(\"POS_\")]\n",
    "features_list_imputed"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BODY_FAT_PCT',\n",
       " 'DISTANCE_KM',\n",
       " 'HAND_LENGTH_CM',\n",
       " 'HAND_WIDTH_CM',\n",
       " 'HEIGHT_CM',\n",
       " 'LANE_AGILITY_TIME_S',\n",
       " 'LOC_X_CM',\n",
       " 'LOC_Y_CM',\n",
       " 'MAX_VERTICAL_LEAP_CM',\n",
       " 'QUARTER_1',\n",
       " 'QUARTER_2',\n",
       " 'QUARTER_3',\n",
       " 'QUARTER_4',\n",
       " 'QUARTER_5',\n",
       " 'QUARTER_6',\n",
       " 'QUARTER_7',\n",
       " 'QUARTER_8',\n",
       " 'REST_D',\n",
       " 'SEASON',\n",
       " 'SHOT_DISTANCE_CM',\n",
       " 'STANDING_REACH_CM',\n",
       " 'STANDING_VERTICAL_LEAP_CM',\n",
       " 'THREE_QUARTER_SPRINT_S',\n",
       " 'TIME_LEFT_S',\n",
       " 'TZ_SHIFT',\n",
       " 'WEIGHT_KG',\n",
       " 'WINGSPAN_CM']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:51.562131Z",
     "start_time": "2026-02-15T19:12:48.259183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# randomize order in master_data_dropped and master_data_imputed\n",
    "master_data_dropped = master_data_dropped.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "master_data_imputed = master_data_imputed.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)"
   ],
   "id": "dc7f8b8dc1a8c20f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:12:54.093661Z",
     "start_time": "2026-02-15T19:12:51.595814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# perform PCA for 90% variance retention on master_data_dropped and master_data_imputed\n",
    "def perform_pca(df, features_list, variance_threshold=0.9):\n",
    "    X = df[features_list]\n",
    "    pca = PCA(n_components=variance_threshold, random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(\n",
    "        f\"PCA reduced {len(features_list)} features to {X_pca.shape[1]} components to retain {variance_threshold * 100}% variance.\")\n",
    "    # print feature names in PCA 1 and PCA 2\n",
    "    print(\"PCA Component 1 explained variance ratio:\", pca.explained_variance_ratio_[0])\n",
    "    print(\"PCA Component 2 explained variance ratio:\", pca.explained_variance_ratio_[1])\n",
    "    return X_pca\n",
    "\n",
    "\n",
    "pca_dropped = perform_pca(master_data_dropped, features_list_dropped)\n",
    "pca_imputed = perform_pca(master_data_imputed, features_list_imputed)\n"
   ],
   "id": "8c271bbf6877eb48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reduced 32 features to 3 components to retain 90.0% variance.\n",
      "PCA Component 1 explained variance ratio: 0.7085438911463618\n",
      "PCA Component 2 explained variance ratio: 0.13519525016841136\n",
      "PCA reduced 27 features to 3 components to retain 90.0% variance.\n",
      "PCA Component 1 explained variance ratio: 0.7082841512138878\n",
      "PCA Component 2 explained variance ratio: 0.13340619831473907\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
