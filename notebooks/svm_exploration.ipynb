{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-15T19:21:50.215614Z",
     "start_time": "2026-02-15T19:21:49.089525Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import requests\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:21:51.426109Z",
     "start_time": "2026-02-15T19:21:51.247937Z"
    }
   },
   "cell_type": "code",
   "source": "RANDOM_STATE = 42",
   "id": "b54f8eb7543cfe71",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:08.309572Z",
     "start_time": "2026-02-15T19:22:00.251775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set to True to download data from GitHub, False to load from local processed directory\n",
    "DOWNLOAD = True\n",
    "\n",
    "if DOWNLOAD:\n",
    "    # Download CSV files from GitHub repository\n",
    "    directory = \"./downloads/\"\n",
    "    filenames = [\n",
    "        \"master_data.parquet\",\n",
    "        \"master_data_dropped.parquet\",\n",
    "        \"master_data_imputed.parquet\"\n",
    "    ]\n",
    "\n",
    "    # Common URL parts\n",
    "    base_url = \"https://github.com/fbec76/sas-curiosity-cup-2026/raw/refs/heads/main/datasets/processed/\"\n",
    "    for fname in filenames:\n",
    "        url = base_url + fname + \"?download=\"\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            with open(directory + fname, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded: {fname}\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {fname}\")\n",
    "    data_dir = \"./downloads/\"\n",
    "    master_data_orig = pd.read_parquet(data_dir + \"master_data.parquet\")\n",
    "    master_data_dropped_orig = pd.read_parquet(data_dir + \"master_data_dropped.parquet\")\n",
    "    master_data_imputed_orig = pd.read_parquet(data_dir + \"master_data_imputed.parquet\")\n",
    "    print(\"Data loaded from downloads directory.\")\n",
    "else:\n",
    "    master_data_orig = pd.read_parquet(\"../datasets/processed/master_data.parquet\")\n",
    "    master_data_dropped_orig = pd.read_parquet(\"../datasets/processed/master_data_dropped.parquet\")\n",
    "    master_data_imputed_orig = pd.read_parquet(\"../datasets/processed/master_data_imputed.parquet\")\n",
    "    data_dir = \"../datasets/processed/\"\n",
    "    print(\"Data loaded from processed directory.\")\n"
   ],
   "id": "89cb2006fff88bfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: master_data.parquet\n",
      "Downloaded: master_data_dropped.parquet\n",
      "Downloaded: master_data_imputed.parquet\n",
      "Data loaded from downloads directory.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:10.845213Z",
     "start_time": "2026-02-15T19:22:10.184596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "master_data = master_data_orig.copy()\n",
    "master_data_dropped = master_data_dropped_orig.copy()\n",
    "master_data_imputed = master_data_imputed_orig.copy()"
   ],
   "id": "8dde1f1155ce433",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:24.976664Z",
     "start_time": "2026-02-15T19:22:24.498479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop GAME_DATE column from master_data_dropped and master_data_imputed\n",
    "master_data_dropped = master_data_dropped.drop(columns=[\"GAME_DATE\"])\n",
    "master_data_imputed = master_data_imputed.drop(columns=[\"GAME_DATE\"])\n",
    "\n",
    "# conver POS_ columns to boolean in master_data_dropped\n",
    "pos_columns = [col for col in master_data_dropped.columns if col.startswith(\"POS_\")]\n",
    "master_data_dropped[pos_columns] = master_data_dropped[pos_columns].astype(bool)"
   ],
   "id": "b7d6bb969e7b0a50",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:26.921076Z",
     "start_time": "2026-02-15T19:22:26.024042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define target variable and features list\n",
    "target_variable = \"MADE_SHOT\"\n",
    "\n",
    "# features list for master_data_dropped\n",
    "cols_to_exclude = [\n",
    "    target_variable, \"GAME_ID\", \"PLAYER_ID\", \"PLAYER_NAME\", \"TEAM_ID\", \"TEAM_NAME\",\n",
    "    \"LAT\", \"LON\", \"D_LAT\", \"D_LON\", \"FLIGHT_TIME_MIN\", \"HOME_TEAM\", \"AWAY_TEAM\", \"IS_3PT\"\n",
    "]\n",
    "\n",
    "features_list = master_data_dropped.columns.difference(cols_to_exclude).tolist()\n",
    "\n",
    "features_list_dropped = features_list\n",
    "features_list_imputed = [col for col in features_list if not col.startswith(\"POS_\")]\n",
    "\n",
    "# Continuous features to normalize (z-score)\n",
    "continuous_features = [\n",
    "    \"BODY_FAT_PCT\",\n",
    "    \"DISTANCE_KM\",\n",
    "    \"HAND_LENGTH_CM\",\n",
    "    \"HAND_WIDTH_CM\",\n",
    "    \"HEIGHT_CM\",\n",
    "    \"LANE_AGILITY_TIME_S\",\n",
    "    \"LOC_X_CM\",\n",
    "    \"LOC_Y_CM\",\n",
    "    \"MAX_VERTICAL_LEAP_CM\",\n",
    "    \"REST_D\",\n",
    "    \"SEASON\",  # normalize to allow learning an overall trend\n",
    "    \"SHOT_DISTANCE_CM\",\n",
    "    \"STANDING_REACH_CM\",\n",
    "    \"STANDING_VERTICAL_LEAP_CM\",\n",
    "    \"THREE_QUARTER_SPRINT_S\",\n",
    "    \"TIME_LEFT_S\",\n",
    "    \"TZ_SHIFT\",\n",
    "    \"WEIGHT_KG\",\n",
    "    \"WINGSPAN_CM\",\n",
    "]\n",
    "\n",
    "# One-hot encode QUARTER\n",
    "if \"QUARTER\" in master_data_dropped.columns:\n",
    "    master_data_dropped = pd.get_dummies(\n",
    "        master_data_dropped,\n",
    "        columns=[\"QUARTER\"],\n",
    "        prefix=\"QUARTER\",\n",
    "        drop_first=False\n",
    "    )\n",
    "\n",
    "if \"QUARTER\" in master_data_imputed.columns:\n",
    "    master_data_imputed = pd.get_dummies(\n",
    "        master_data_imputed,\n",
    "        columns=[\"QUARTER\"],\n",
    "        prefix=\"QUARTER\",\n",
    "        drop_first=False\n",
    "    )\n",
    "\n",
    "# Ensure we only scale columns that exist (robust to missing columns)\n",
    "continuous_features_present = [c for c in continuous_features if c in master_data_dropped.columns]\n",
    "\n",
    "# Rebuild features_list after one-hot encoding QUARTER (new columns added)\n",
    "features_list = master_data_dropped.columns.difference(cols_to_exclude).tolist()\n",
    "\n",
    "features_list_dropped = features_list\n",
    "features_list_imputed = [col for col in features_list if not col.startswith(\"POS_\")]"
   ],
   "id": "d47e73b261dcb1e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:29.418588Z",
     "start_time": "2026-02-15T19:22:29.231985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def svm_classifier_with_scaling(\n",
    "        df,\n",
    "        target,\n",
    "        features_list,\n",
    "        continuous_features,\n",
    "        cv_folds=5,\n",
    "        scoring=\"f1\",\n",
    "        kernel=\"rbf\",  # \"rbf\" or \"poly\"\n",
    "):\n",
    "    X = df[features_list]\n",
    "    y = df[target]\n",
    "\n",
    "    start_time = pd.Timestamp.now()\n",
    "\n",
    "    if kernel not in {\"rbf\", \"poly\"}:\n",
    "        raise ValueError(\"kernel must be 'rbf' or 'poly'\")\n",
    "\n",
    "    # Only scale columns that are actually used and exist\n",
    "    continuous_present = [c for c in continuous_features if c in features_list]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), continuous_present),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"svc\", SVC(kernel=kernel)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if kernel == \"rbf\":\n",
    "        param_grid = {\n",
    "            \"svc__C\": [0.1, 1, 10, 100],\n",
    "            \"svc__gamma\": [\"scale\", \"auto\", 1e-3, 1e-2, 1e-1],\n",
    "        }\n",
    "    else:  # poly\n",
    "        param_grid = {\n",
    "            \"svc__C\": [0.1, 1, 10, 100],\n",
    "            \"svc__gamma\": [\"scale\", \"auto\", 1e-3, 1e-2, 1e-1],\n",
    "            \"svc__degree\": [2, 3, 4, 5],\n",
    "            \"svc__coef0\": [0.0, 0.5, 1.0],\n",
    "        }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_folds,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # CV predictions from the selected (tuned) pipeline\n",
    "    y_pred_cv = cross_val_predict(best_model, X, y, cv=cv_folds, n_jobs=-1)\n",
    "\n",
    "    print(\"Scaled continuous columns (present):\", continuous_present)\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "    print(\"Classification Report (CV preds):\\n\", classification_report(y, y_pred_cv))\n",
    "    print(\"Confusion Matrix (CV preds):\\n\", confusion_matrix(y, y_pred_cv))\n",
    "    print(\"Time taken:\", pd.Timestamp.now() - start_time)\n",
    "\n",
    "    return best_model"
   ],
   "id": "6f5121321bced9f6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:22:32.964694Z",
     "start_time": "2026-02-15T19:22:30.705326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# randomize order in master_data_dropped and master_data_imputed\n",
    "master_data_dropped = master_data_dropped.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "master_data_imputed = master_data_imputed.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)"
   ],
   "id": "5d737be4d81a4e6e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-15T19:29:46.778955Z",
     "start_time": "2026-02-15T19:25:43.062631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run SVM with RBF kernel on 25% of master_data_dropped\n",
    "best_svm_rbf = svm_classifier_with_scaling(\n",
    "    df=master_data_dropped.sample(frac=0.01, random_state=RANDOM_STATE),\n",
    "    target=target_variable,\n",
    "    features_list=features_list_dropped,\n",
    "    continuous_features=continuous_features,\n",
    "    cv_folds=2,\n",
    "    scoring=\"f1\",\n",
    "    kernel=\"rbf\",\n",
    ")"
   ],
   "id": "9513b9e782c160fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled continuous columns (present): ['BODY_FAT_PCT', 'DISTANCE_KM', 'HAND_LENGTH_CM', 'HAND_WIDTH_CM', 'HEIGHT_CM', 'LANE_AGILITY_TIME_S', 'LOC_X_CM', 'LOC_Y_CM', 'MAX_VERTICAL_LEAP_CM', 'REST_D', 'SEASON', 'SHOT_DISTANCE_CM', 'STANDING_REACH_CM', 'STANDING_VERTICAL_LEAP_CM', 'THREE_QUARTER_SPRINT_S', 'TIME_LEFT_S', 'TZ_SHIFT', 'WEIGHT_KG', 'WINGSPAN_CM']\n",
      "Best Hyperparameters: {'svc__C': 1, 'svc__gamma': 0.001}\n",
      "Classification Report (CV preds):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.58      0.60     13834\n",
      "        True       0.55      0.59      0.57     12112\n",
      "\n",
      "    accuracy                           0.58     25946\n",
      "   macro avg       0.58      0.58      0.58     25946\n",
      "weighted avg       0.58      0.58      0.58     25946\n",
      "\n",
      "Confusion Matrix (CV preds):\n",
      " [[8001 5833]\n",
      " [5015 7097]]\n",
      "Time taken: 0 days 00:04:03.388683\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run SVM with RBF kernel on 25% of master_data_imputed\n",
    "best_svm_rbf_imputed = svm_classifier_with_scaling(\n",
    "    df=master_data_imputed.sample(frac=0.25, random_state=RANDOM_STATE),\n",
    "    target=target_variable,\n",
    "    features_list=features_list_imputed,\n",
    "    continuous_features=continuous_features,\n",
    "    cv_folds=5,\n",
    "    scoring=\"f1\",\n",
    "    kernel=\"rbf\",\n",
    ")"
   ],
   "id": "15ed44d2a283cfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run SVM with polynomial kernel on 25% of master_data_dropped\n",
    "best_svm_poly = svm_classifier_with_scaling(\n",
    "    df=master_data_dropped.sample(frac=0.25, random_state=RANDOM_STATE),\n",
    "    target=target_variable,\n",
    "    features_list=features_list_dropped,\n",
    "    continuous_features=continuous_features,\n",
    "    cv_folds=5,\n",
    "    scoring=\"f1\",\n",
    "    kernel=\"poly\",\n",
    ")"
   ],
   "id": "2e55445ced55fc81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run SVM with polynomial kernel on 25% of master_data_imputed\n",
    "best_svm_poly_imputed = svm_classifier_with_scaling(\n",
    "    df=master_data_imputed.sample(frac=0.25, random_state=RANDOM_STATE),\n",
    "    target=target_variable,\n",
    "    features_list=features_list_imputed,\n",
    "    continuous_features=continuous_features,\n",
    "    cv_folds=5,\n",
    "    scoring=\"f1\",\n",
    "    kernel=\"poly\",\n",
    ")"
   ],
   "id": "28d11dd4cb1c15bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
